nohup: ignoring input
[{'tailEntity': {'type': 'uri', 'value': 'http://rdf.freebase.com/ns/m.048bcbz'}}]
[]
[]
'After Hitler_
'After Hitler\
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]
Device set to use cuda:0
  The team that won the 2009 AFC Championship Game was the New York Jets, and their head coach at the time was Rex Ryan.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]
Device set to use cuda:0
====================================================================================
question 0 : where is tyrese gibson from?
topics: [('m.01l1b90', 'Tyrese Gibson')]
grounds: ['Watts']
========================================================
depth:  0
<relation candidates>
[['base.popstra.celebrity.canoodled', 'tv.tv_actor.guest_roles', 'music.group_member.membership', 'award.award_winner.awards_won', 'award.award_honor.award_winner', 'freebase.valuenotation.has_value', 'common.topic.article', 'film.performance.actor', 'user.narphorium.people.nndb_person.nndb_id', 'people.person.place_of_birth', 'music.composition.composer', 'book.author.works_written', 'base.popstra.celebrity.wears', 'common.topic.alias', 'film.actor.film', 'tv.tv_guest_role.actor', 'base.popstra.sww_base.heat', 'film.personal_film_appearance.person', 'people.person.places_lived', 'music.recording_contribution.contributor']]
Traceback (most recent call last):
  File "/home/shcha/PaperImplementation/ThinkOnGraph/implementation/evaluate.py", line 72, in <module>
    (answer, paths, working) = model.inference(question, topicEntities)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shcha/PaperImplementation/ThinkOnGraph/implementation/toG.py", line 32, in inference
    topNRelations = self.llm.relationPrune(question, paths, relationCandidates)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shcha/PaperImplementation/ThinkOnGraph/implementation/llm.py", line 94, in relationPrune
    answer = self.llama.answer(prompt, 0.4)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shcha/PaperImplementation/ThinkOnGraph/implementation/llm.py", line 37, in answer
    outputs = generator([{"role": "user", "content": prompt},], max_length=256)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shcha/miniconda3/lib/python3.12/site-packages/transformers/pipelines/text_generation.py", line 278, in __call__
    return super().__call__(Chat(text_inputs), **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shcha/miniconda3/lib/python3.12/site-packages/transformers/pipelines/base.py", line 1362, in __call__
    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shcha/miniconda3/lib/python3.12/site-packages/transformers/pipelines/base.py", line 1369, in run_single
    model_outputs = self.forward(model_inputs, **forward_params)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shcha/miniconda3/lib/python3.12/site-packages/transformers/pipelines/base.py", line 1269, in forward
    model_outputs = self._forward(model_inputs, **forward_params)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shcha/miniconda3/lib/python3.12/site-packages/transformers/pipelines/text_generation.py", line 383, in _forward
    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shcha/miniconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/shcha/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py", line 2108, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/home/shcha/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py", line 1411, in _validate_generated_length
    raise ValueError(
ValueError: Input length of input_ids is 708, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.
